{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc891384",
   "metadata": {},
   "source": [
    "# NMNH data in GBIF\n",
    "\n",
    "This workbook collects data from GBIF, Crossref, Altmetrics, and OpenAlex as the\n",
    "basis for a Tableau dashboard about NMNH data in GBIF. API calls are confined to\n",
    "this workbook, but the resulting data is further processed in 2-combine-data.ipynb."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "\n",
    "DEBUG = None\n",
    "EMAIL = \"\"\n",
    "ORG_UUID = \"bc092ff0-02e4-11dc-991f-b8a03c50a862\"  # NMNH organization UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6055a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(*args, **kwargs):\n",
    "    \"\"\"Performs a GET request with a brief delay for fresh requests\"\"\"\n",
    "    resp = session.get(*args, **kwargs)\n",
    "    if not resp.from_cache:\n",
    "        print(resp.status_code, resp.url)\n",
    "        time.sleep(0.1)\n",
    "    return resp\n",
    "\n",
    "\n",
    "def get_gbif(*args, **kwargs):\n",
    "    \"\"\"Gets results from all pages of a GBIF request\"\"\"\n",
    "    results = []\n",
    "    while True:\n",
    "        rec = get(*args, **kwargs).json()\n",
    "        results.extend(rec[\"results\"])\n",
    "        if rec.get(\"endOfRecords\", True):\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "        try:\n",
    "            kwargs[\"params\"][\"offset\"] += rec[\"limit\"]\n",
    "        except KeyError:\n",
    "            kwargs.setdefault(\"params\", {})[\"offset\"] = rec[\"limit\"]\n",
    "    return {\n",
    "        \"offset\": 0,\n",
    "        \"limit\": rec[\"count\"],\n",
    "        \"endOfRecords\": True,\n",
    "        \"count\": rec[\"count\"],\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_fn(resp):\n",
    "    \"\"\"Caches requests with 200 or 404 with certain text\"\"\"\n",
    "    cache_404s = {\n",
    "        \"doi not found\",\n",
    "        \"not found\",\n",
    "        \"<!doctype html>\\n<html lang=en>\\n<title>404 not found</title>\\n<h1>not found</h1>\\n<p>the requested url was not found on the server. if you entered the url manually please check your spelling and try again.</p>\",\n",
    "    }\n",
    "    return (\n",
    "        resp.status_code == 200\n",
    "        or resp.status_code == 404\n",
    "        and resp.text.strip().lower() in cache_404s\n",
    "    )\n",
    "\n",
    "\n",
    "def is_paleo(row):\n",
    "    \"\"\"Simplistically check if paleo data\"\"\"\n",
    "    val = str(row).lower()\n",
    "    return \"fossil\" in val or \"paleo\" in val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea445d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache records with 200 status code or 404s that indicate a record was not found\n",
    "session = requests_cache.CachedSession(allowable_codes=(200, 404), filter_fn=filter_fn)\n",
    "\n",
    "if not EMAIL:\n",
    "    raise ValueError(\"You must specify an email using the EMAIL constant\")\n",
    "headers = {\"User-Agent\": f\"python-requests/{requests.__version__}/{EMAIL}\"}\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "lit_dir = data_dir / \"literature\"\n",
    "lit_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SRO data. Note that you must be on the domain to run this one.\n",
    "resp = get(\n",
    "    \"https://staff.research.si.edu/export/srb_search_export_action.cfm\",\n",
    "    params={\n",
    "        \"search_term\": \"\",\n",
    "        \"submit\": \"Export data\",\n",
    "        \"format\": \"JSON\",\n",
    "        \"Unit\": \"330000\",\n",
    "        \"count\": 100000,\n",
    "    },\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "# The JSON provided by SRO includes imporperly escaped double quotes. Clear those\n",
    "# via a regex, first clearing pairs of quotes, then single quotes.\n",
    "text = re.sub('(: \".*?)\"(.*?)\"(.*?\")', r\"\\1\\2\\3\", resp.text)\n",
    "text = re.sub('(: \".*?)\"(.*?\")', r\"\\1\\2\", text)\n",
    "\n",
    "sro = pd.DataFrame([list(i[\"reference\"].values())[0] for i in json.loads(text)])[\n",
    "    [\"doi\"]\n",
    "]\n",
    "sro[\"doi\"] = sro[\"doi\"].str.split(\"/\", n=3, expand=True)[3]\n",
    "sro = sro[~pd.isna(sro[\"doi\"])]\n",
    "sro.to_csv(\"data/sro.csv\", index=False)\n",
    "sro_dois = set(sro[\"doi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd13198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GBIF datasets published by NMNH\n",
    "rec = get_gbif(\n",
    "    \"https://api.gbif.org/v1/dataset/search\",\n",
    "    params={\"publishingOrg\": ORG_UUID},\n",
    "    headers=headers,\n",
    "    expire_after=7 * 24 * 60 * 60,\n",
    ")\n",
    "\n",
    "datasets = pd.DataFrame(rec[\"results\"])\n",
    "datasets = datasets[datasets[\"title\"].str.contains(\"NMNH\")]\n",
    "datasets.to_csv(data_dir / \"datasets.csv\", index=False)\n",
    "org_title = datasets.iloc[0][\"publishingOrganizationTitle\"]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats for all downloads\n",
    "rows = []\n",
    "for key in datasets[\"key\"]:\n",
    "    # Request cached for one week\n",
    "    rec = get_gbif(\n",
    "        \"https://api.gbif.org/v1/occurrence/download/statistics\",\n",
    "        params={\"datasetKey\": key},\n",
    "        expire_after=7 * 24 * 60 * 60,\n",
    "    )\n",
    "    rows.extend(rec[\"results\"])\n",
    "\n",
    "stats = pd.DataFrame(rows)\n",
    "stats = stats.groupby([\"year\"]).sum()[[\"numberDownloads\", \"totalRecords\"]].reset_index()\n",
    "stats.to_csv(\"data/downloads_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get literature citing each dataset\n",
    "for key in datasets[\"key\"]:\n",
    "    # Request cached for one week\n",
    "    resp = session.get(\n",
    "        \"https://api.gbif.org/v1/literature/export\",\n",
    "        params={\"format\": \"CSV\", \"gbifDatasetKey\": key},\n",
    "        headers=headers,\n",
    "        expire_after=7 * 24 * 60 * 60,\n",
    "    )\n",
    "    with open(lit_dir / (key + \".csv\"), \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "# Filter non-paleo papers from the paleo dataset\n",
    "path = lit_dir / \"c8681cc2-9d0a-4c5f-b620-5c753abfe2bc.csv\"\n",
    "pd.DataFrame([r for _, r in pd.read_csv(path).iterrows() if is_paleo(r)]).to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get citations for each publication with a DOI\n",
    "for path in lit_dir.glob(\"*.csv\"):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df = pd.read_csv(path).sort_values(\"gbif_download_key\")\n",
    "\n",
    "    # If a dataset has not been cited, the literature CSV will be empty. Remove\n",
    "    # these files.\n",
    "    if not len(df):\n",
    "        path.unlink()\n",
    "        continue\n",
    "\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        # For records that specify a DOI (which is most of them), use the DOI to\n",
    "        # grab a citation and an Altmetric score.\n",
    "        if not pd.isna(row[\"identifiers\"]):\n",
    "            for val in row[\"identifiers\"].split(\"|\"):\n",
    "                if val.startswith(\"10.\"):\n",
    "\n",
    "                    # Check if DOI appears in SRO\n",
    "                    row[\"sro\"] = val in sro_dois\n",
    "\n",
    "                    # Get citation from Crossref\n",
    "                    resp = get(\n",
    "                        \"https://citation.doi.org/format\",\n",
    "                        params={\n",
    "                            \"doi\": val,\n",
    "                            \"style\": \"apa\",\n",
    "                            \"lang\": \"en-US\",\n",
    "                        },\n",
    "                        headers=headers,\n",
    "                    )\n",
    "                    if resp.text != \"DOI not found\":\n",
    "                        row[\"citation\"] = resp.text.strip()\n",
    "\n",
    "                    # Get citation count from OpenAlex\n",
    "                    resp = get(\n",
    "                        f\"https://api.openalex.org/works/doi:{val}\",\n",
    "                        params={\n",
    "                            \"select\": \"cited_by_count,citation_normalized_percentile\"\n",
    "                        },\n",
    "                        headers=headers,\n",
    "                    )\n",
    "                    if not resp.text.startswith(\"<\"):\n",
    "                        row[\"cited_by_count\"] = resp.json()[\"cited_by_count\"]\n",
    "\n",
    "                    # Get Altmetric score. Altmetric only allow 1200 calls per day\n",
    "                    # to its counts-only endpoint without an API key, so this may\n",
    "                    # take some time.\n",
    "                    resp = get(\n",
    "                        f\"https://api.altmetric.com/v1/doi/{val}\",\n",
    "                        headers=headers,\n",
    "                    )\n",
    "                    if resp.status_code not in (200, 404):\n",
    "                        # The API is supposed to throw a 429 error if you exceed the\n",
    "                        # daily limit. I've never seen it, but this should kill the\n",
    "                        # script if it shows up.\n",
    "                        raise ValueError(f\"Invalid response: {resp.headers}\")\n",
    "                    if resp.text != \"Not Found\":\n",
    "                        data = resp.json()\n",
    "                        row[\"altmetric_id\"] = data[\"details_url\"].split(\"=\")[-1]\n",
    "                        row[\"altmetric_score\"] = resp.json()[\"score\"]\n",
    "\n",
    "                    break\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        if time.time() - start_time >= 5:\n",
    "            print(f\"{len(rows):,} works processed\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(f\"{len(rows):,} works processed\")\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for GBIF downloads\n",
    "keys = {}\n",
    "downloads = []\n",
    "start_time = time.time()\n",
    "no_nmnh_records = {}\n",
    "for path in lit_dir.glob(\"*.csv\"):\n",
    "    print(f\"Processing {path.name}...\")\n",
    "    df = pd.read_csv(path).sort_values(\"gbif_download_key\")\n",
    "    notify = 1000\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        # Filter non-paleo works from the paleo dataset\n",
    "        if path.stem == \"c8681cc2-9d0a-4c5f-b620-5c753abfe2bc\" and not is_paleo(row):\n",
    "            continue\n",
    "\n",
    "        # Get additional info about each linked download\n",
    "        has_nmnh = []\n",
    "        if not pd.isna(row[\"gbif_download_key\"]):\n",
    "\n",
    "            for key in row[\"gbif_download_key\"].split(\"|\"):\n",
    "\n",
    "                if DEBUG is not None and key != DEBUG:\n",
    "                    continue\n",
    "\n",
    "                # The same download may be associated with multiple datasets, so\n",
    "                # only check each key once. It does not seem to be possible to\n",
    "                # determine which dataset records come from if multiple datasets are\n",
    "                # associated with a download.\n",
    "                try:\n",
    "                    keys[key]\n",
    "                except KeyError:\n",
    "                    keys[key] = None\n",
    "                else:\n",
    "                    has_nmnh.append(key)\n",
    "                    continue\n",
    "\n",
    "                download = {}\n",
    "\n",
    "                # Get number of NMNH specimens in the dataset\n",
    "                data = get_gbif(\n",
    "                    f\"https://api.gbif.org/v1/occurrence/download/{key}/organizations\",\n",
    "                    params={\"organizationTitle\": org_title},\n",
    "                )\n",
    "                for result in data[\"results\"]:\n",
    "                    if result[\"organizationKey\"] == ORG_UUID:\n",
    "                        download[\"nmnhRecords\"] = result[\"numberRecords\"]\n",
    "                        break\n",
    "                else:\n",
    "                    # Omit datasets that do not include NMNH records\n",
    "                    continue\n",
    "\n",
    "                has_nmnh.append(key)\n",
    "\n",
    "                # Get general information about the dataset\n",
    "                resp = get(\n",
    "                    f\"https://api.gbif.org/v1/occurrence/download/{key}\",\n",
    "                    headers=headers,\n",
    "                )\n",
    "                download.update(\n",
    "                    {\n",
    "                        k: json.dumps(v) if isinstance(v, (bool, dict, list)) else v\n",
    "                        for k, v in resp.json().items()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Counts for SPECIES_LIST downloads are for the number of taxa, not\n",
    "                # the number of occurrences. We can use the organizations endpoint to\n",
    "                # get the number of occurrences. Not positive this is the best\n",
    "                # approach, but it is consistent with the other download types.\n",
    "                if \"SPECIES_LIST\" in str(download):\n",
    "                    data = get_gbif(\n",
    "                        f\"https://api.gbif.org/v1/occurrence/download/{key}/organizations\",\n",
    "                    )\n",
    "                    download[\"totalRecords\"] = sum(\n",
    "                        [r[\"numberRecords\"] for r in data[\"results\"]]\n",
    "                    )\n",
    "\n",
    "                downloads.append(download)\n",
    "\n",
    "                if time.time() - start_time >= 5:\n",
    "                    print(f\"{len(keys):,} downloads processed\")\n",
    "                    start_time = time.time()\n",
    "\n",
    "                if not resp.from_cache:\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "        # Note papers that do not seem to link to any NMNH records for further review\n",
    "        # NOTE: Discussed these with GBIF. Do not need to flag these records.\n",
    "        # if not has_nmnh:\n",
    "        #    key = row[\"identifiers\"]\n",
    "        #    if pd.isna(key):\n",
    "        #        key = row[\"title\"]\n",
    "        #    try:\n",
    "        #        no_nmnh_records[key]\n",
    "        #    except KeyError:\n",
    "        #        no_nmnh_records[key] = True\n",
    "        #        print(f\"{len(no_nmnh_records)}. {key}\")\n",
    "\n",
    "print(f\"{len(keys):,} downloads processed\")\n",
    "\n",
    "downloads = pd.DataFrame(downloads).drop_duplicates()\n",
    "downloads.to_csv(data_dir / \"downloads_published.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}