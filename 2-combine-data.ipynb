{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075dcc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_citation(row):\n",
    "    \"\"\"Build citation for row with a DOI\"\"\"\n",
    "    authors = row[\"authors\"] if not pd.isna(row[\"authors\"]) else \"\"\n",
    "    authors = re.sub(\"^null \", \"\", authors).replace(\"|\", \", \")\n",
    "    return f\"{authors} ({row['published'][:4]}). {row['title']}\"\n",
    "\n",
    "def get_categories(pred, queried=None):\n",
    "    return sorted({cats[f] for f in get_fields(pred, queried=queried)})\n",
    "\n",
    "def get_doi(val):\n",
    "    \"\"\"Gets the first DOI from a pipe-delimited list\"\"\"\n",
    "    try:\n",
    "        for val in val.split(\"|\"):\n",
    "            if val.startswith(\"10.\"):\n",
    "                return val\n",
    "        return np.nan\n",
    "    except (AttributeError, IndexError):\n",
    "        return np.nan\n",
    "\n",
    "def get_fields(pred, queried=None):\n",
    "    \"\"\"Get the filters applied to a GBIF download\"\"\"\n",
    "    if isinstance(pred, str):\n",
    "        pred = json.loads(pred)\n",
    "    pred = pred.get(\"predicate\", pred)\n",
    "    if queried is None:\n",
    "        queried = {}\n",
    "    try:\n",
    "        key = pred[\"key\"]\n",
    "        try:\n",
    "            val = [pred[\"value\"]]\n",
    "        except KeyError:\n",
    "            val = pred[\"values\"]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        queried.setdefault(key, []).extend(val)\n",
    "    for pred in pred.get(\"predicates\", []):\n",
    "        get_fields(pred, queried)\n",
    "    return list(queried)\n",
    "\n",
    "def get_multivalue(df):\n",
    "    \"\"\"Get pipe-delimited columns in a dataframe\"\"\"\n",
    "    keys = []\n",
    "    for key in df.columns:\n",
    "        is_multivalue = any((isinstance(s, list) for s in df[key]))\n",
    "        if not is_multivalue:\n",
    "            try:\n",
    "                is_multivalue = df[key].str.contains(\"|\", regex=False).any()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        if is_multivalue:\n",
    "            keys.append(key)\n",
    "    return keys\n",
    "\n",
    "def format_keywords(val):\n",
    "    \"\"\"Titlecase and remove underscores from a value\"\"\"\n",
    "    return val.replace(\"_\", \" \").title()\n",
    "\n",
    "def format_list(vals):\n",
    "    \"\"\"Join and remove underscores from a list of values\"\"\"\n",
    "    return \"|\".join(vals).upper().replace(\" \", \"_\")\n",
    "\n",
    "def split_multivalue(df, key, field):\n",
    "    \"\"\"Split a multivalue field in a dataframe into multiple rows\"\"\"\n",
    "\n",
    "    def as_list(val):\n",
    "        if isinstance(val, list) or pd.isna(val):\n",
    "            return val\n",
    "        if isinstance(val, (int, float)):\n",
    "            return [val]\n",
    "        if not val:\n",
    "            return []\n",
    "        if isinstance(val, str):\n",
    "            return val.split(\"|\")\n",
    "        return list(val)\n",
    "\n",
    "    df2 = df[[key, field]].dropna().copy()\n",
    "    df2[field] = df2[field].apply(as_list)\n",
    "    df2.explode(field).drop_duplicates().to_csv(f\"output/sub_{field}.csv\", index=False)\n",
    "    del df[field]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\")\n",
    "KEEP = {\"gbif_download_key\", \"dat_id\"}\n",
    "\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"categories.yml\") as f:\n",
    "    cats = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa581681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all downloads\n",
    "shutil.copy2(\"data/downloads_all.csv\", \"output/downloads_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine literature for all NMNH datasets\n",
    "dfs = []\n",
    "for path in (data_dir / \"literature\").glob(\"*.csv\"):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"dataset_id\"] = path.stem\n",
    "    dfs.append(df)\n",
    "lit = pd.concat(dfs)\n",
    "\n",
    "# Get the DOI for all publications\n",
    "lit[\"doi\"] = lit[\"identifiers\"].apply(get_doi)\n",
    "\n",
    "lit[\"dataset\"] = lit[\"citation\"].str.contains(\"[Dataset]\", regex=False)\n",
    "\n",
    "# Truncate the author list\n",
    "def truncate_authors(val):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val = val.strip()\n",
    "    try:\n",
    "        authors, year, rest = re.split(r\"( *\\(\\d{4}\\))\", val, 1)\n",
    "    except ValueError:\n",
    "        return val\n",
    "    else:\n",
    "        if authors.count(\",\") <= 2:\n",
    "            return val\n",
    "        return authors.split(\",\")[0] + \" et al.\" + year + rest\n",
    "\n",
    "lit[\"citation\"] = lit[\"citation\"].apply(truncate_authors)\n",
    "\n",
    "cond = pd.isna(lit[\"citation\"])\n",
    "lit.loc[cond, \"citation\"] = lit.loc[cond].apply(build_citation, axis=1)\n",
    "\n",
    "lit[\"sro\"] = lit[\"sro\"].fillna(False)\n",
    "\n",
    "# Remove extraneous fields\n",
    "lit = lit[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"doi\",\n",
    "        \"citation\",\n",
    "        \"gbif_download_key\",\n",
    "        \"published\",\n",
    "        \"dataset\",\n",
    "        \"open_access\",\n",
    "        \"peer_review\",\n",
    "        \"topics\",\n",
    "        \"sro\",\n",
    "        \"cited_by_count\",\n",
    "        \"altmetric_score\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Create separate tables for multivalue fields\n",
    "for key in get_multivalue(lit):\n",
    "    lit = split_multivalue(lit, \"id\", key)\n",
    "\n",
    "# Drop duplicate rows and save\n",
    "lit = lit.drop_duplicates()\n",
    "lit.to_csv(\"output/literature.csv\", index=False)\n",
    "lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read occurrence metadata. These are custom downloads. Fields use file prefix.\n",
    "downloads = pd.read_csv(data_dir / \"downloads_published.csv\")\n",
    "downloads = downloads.rename(\n",
    "    columns={\"key\": \"gbif_download_key\", \"doi\": \"gbif_download_doi\"}\n",
    ")\n",
    "\n",
    "downloads[\"format\"] = downloads[\"request\"].apply(lambda r: json.loads(r)[\"format\"])\n",
    "\n",
    "downloads[\"fieldsQueried\"] = downloads[\"request\"].apply(get_fields)\n",
    "downloads.loc[~downloads[\"fieldsQueried\"].astype(bool), \"fieldsQueried\"] = np.nan\n",
    "\n",
    "downloads[\"topicsQueried\"] = downloads[\"request\"].apply(get_categories)\n",
    "downloads.loc[~downloads[\"topicsQueried\"].astype(bool), \"topicsQueried\"] = np.nan\n",
    "\n",
    "downloads = downloads[\n",
    "    [\n",
    "        \"gbif_download_key\",\n",
    "        \"gbif_download_doi\",\n",
    "        \"created\",\n",
    "        \"format\",\n",
    "        \"fieldsQueried\",\n",
    "        \"topicsQueried\",\n",
    "        \"numberDatasets\",\n",
    "        \"nmnhRecords\",\n",
    "        \"totalRecords\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Split multivalue fields into separate tables\n",
    "for key in get_multivalue(downloads):\n",
    "    downloads = split_multivalue(downloads, \"gbif_download_key\", key)\n",
    "\n",
    "# Drop duplicate rows and save\n",
    "downloads = downloads.drop_duplicates()\n",
    "downloads.to_csv(\"output/downloads_published.csv\", index=False)\n",
    "downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0113d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated based on an email from the GBIF help desk but kept for posterity. \n",
    "# The short reason for why publications are linked to NMNH without inlcuding any\n",
    "# downloads with NMNH records is that GBIF uses other data to link records to\n",
    "# providers besides download keys. For example, a paper may use data from a previous\n",
    "# paper that was itself based on GBIF data.\n",
    "\n",
    "# The link table includes downloads that do not have any NMNH records. Remove those here.\n",
    "\n",
    "#vals = set(downloads[\"gbif_download_key\"])\n",
    "#for path in Path(\"output\").glob(\"sub_*.csv\"):\n",
    "#    links = pd.read_csv(path)\n",
    "#    if \"gbif_download_key\" in links.columns:\n",
    "#        links = links[links[\"gbif_download_key\"].isin(vals)]\n",
    "#        links.to_csv(path, index=False)\n",
    "\n",
    "#vals = set(lit[\"id\"])\n",
    "#for path in Path(\"output\").glob(\"sub_*.csv\"):\n",
    "#    links = pd.read_csv(path)\n",
    "#    if \"id\" in links.columns:\n",
    "#        links = links[links[\"id\"].isin(vals)]\n",
    "#        links.to_csv(path, index=False)\n",
    "\n",
    "# The GBIF literature exports for NMNH datasets include several hundred papers that\n",
    "# do not appear to cite datasets featuring NMNH specimens (although this conclusion\n",
    "# is based on the results of another GBIF API). Moreover, there are incongruities\n",
    "# between the information from the export API and the literature API, for example\n",
    "# literature records that do not specify a GBIF download. For now, remove all papers\n",
    "# that cannot be associated with a download.\n",
    "# links = pd.read_csv(\"output/sub_gbif_download_key.csv\")\n",
    "# lit[lit[\"id\"].isin(links[\"id\"])].to_csv(\"output/literature.csv\", index=False)\n",
    "\n",
    "# These are the records that do not appear to be associated with datasets with\n",
    "# NMNH specimens.\n",
    "# lit[~lit[\"id\"].isin(links[\"id\"])]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}